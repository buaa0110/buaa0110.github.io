<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Haotong Qin (秦浩桐)</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="https://htqin.github.io/Imgs/buaa_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Haotong Qin (秦浩桐)</name></p>
                <p align="justify">I am a PhD student (2019.09-) in the State Key Laboratory of Software Development Environment (SKLSDE)
                    and Shen Yuan Honors College at <a href="https://www.buaa.edu.cn/">Beihang University</a>,
                    supervised by Prof. <a href="http://sites.nlsde.buaa.edu.cn/~liwei/">Wei Li</a>
                    and Prof. <a href="http://sites.nlsde.buaa.edu.cn/~xlliu/">Xianglong Liu</a>.
                    I obtained my BSc degree in Computer Science and Engineering (<i>Summa Cum Laude</i>) from <a href="https://www.buaa.edu.cn/">Beihang University</a> in 2019.
                    <br><br>
                    In my PhD study, I interned at the WeiXin Group (WXG) of <a href="https://www.tencent.com/en-us">Tencent</a> (Shenzhen, China). 
                    In my undergraduate study, I interned at the Speech Group of Microsoft Research Asia (<a href="https://www.msra.cn/">MSRA</a>) (Beijing,
                    China) and the Big Data Intelligence Group on SmartCity (<a href="https://www.bigscity.com/">BIGSCity</a>) of Beihang University (Beijing, China).
                    <br><br>
                    <strong>Email:</strong> qinhaotong@nlsde.buaa.edu.cn / qinhaotong@buaa.edu.cn
                <br>

                </p><p align="center">
                    <a href="https://htqin.github.io/CV/cv_htqin.pdf">CV</a> /
                    <a href="https://github.com/htqin"> Github </a> / 
                    <a href="https://www.linkedin.com/in/haotongqin/">Linkedin</a> / 
                    <a href="https://scholar.google.com/citations?user=mK6n-KgAAAAJ&hl=zh-CN">Google Scholar</a> / 
                    <a href="https://www.zhihu.com/column/c_1343181023409074176">知乎专栏</a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./Imgs/photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">I'm interested in hardware-friendly deep learning. 
               And my research goal is to enable state-of-the-art neural network models to be deployed on resource-limited hardware, 
               which includes the compression and acceleration for multiple architectures, 
               and the flexible and efficient deployment on multiple hardware. 
            </p>
               My research focus is mainly on:
                <ul>
                    <li>
                       Network binarization and quantization
                    </li>
                    <li>
                        Neural architecture design and search
                    </li>
                    <li>
                        Hardware implementation of compact network
                    </li>
                    <li>
                        Image synthesizing
                    </li>
                    <li>
                        3D point cloud processing
                    </li>
                    <li>
                        Few/Zero-shot learning
                    </li>
                </ul>
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading> 
            <p> <strong>[2021.05.17]</strong> Obtained Huawei Scholarship (<font color="red">Top1%</font>).</p>
            <p> <strong>[2021.03.01]</strong> One co-first-authored <font color="red"><strong>oral</strong></font> <a href="https://arxiv.org/pdf/2103.01049.pdf">paper</a> for data-free quantization is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
            <p> <strong>[2021.01.13]</strong> One first-authored <a href="https://openreview.net/pdf?id=9QLRCVysdlO">paper</a> for PointNet binarization is accepted by <a href="https://iclr.cc/">ICLR 2021</a>. 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=BiPointNet&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe></p>
            <p> <strong>[2020.09.20]</strong> Obtained China National Scholarship (<font color="red">Top2%</font>).</p>
            <p> <strong>[2020.09.18]</strong> Released our open source project <a href="https://github.com/htqin/awesome-model-quantization">"Awesome Model Quantization"</a>.
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=awesome-model-quantization&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe></p>
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications / Preprints</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>

        <tr><td width="20%"><img src="./Imgs/bipointnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2010.05501">
                <papertitle>BiPointNet: Binary Neural Network for Point Clouds</papertitle></a>
                [<a href="https://arxiv.org/pdf/2010.05501.pdf">PDF</a>]
                <br><strong>Haotong Qin*</strong>, Zhongang Cai*, Mingyuan Zhang*, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su  (* indicates equal contribution)
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2010.05501">arXiv</a> / 
                <a href="https://htqin.github.io/Projects/BiPointNet.html"><font color="red">Project</font></a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/W13lZOWAzw03PqafTteP_Q"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/u84BaMXXJyujKM-Oaj0HBQ"><font color="red">商汤学术) </font></a> /
                <a href="https://github.com/htqin/BiPointNet"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We presented BiPointNet, the first model binarization approach for efficient deep learning on point clouds. 
                    BiPointNet gave an impressive 14.7× speedup and 18.9× storage saving on real-world resource-constrained devices.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/dsg.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2103.01049">
                <papertitle>Diversifying Sample Generation for Data-Free Quantization</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.01049.pdf">PDF</a>]
                <br>Xiangguo Zhang*, <strong>Haotong Qin*</strong>, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu  (* indicates equal contribution)
                <br>
                <em style="font-size:13px">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> <font color="red"><strong style="font-size:13px">Oral</strong></font>, <font style="font-size:13px">2021</font>
                <br>
                <a href="https://arxiv.org/abs/2103.01049">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/0qIBM4wJTc12WzghV1V_gQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/WftpvEWa_BAyljyyRSIEVQ"><font color="red">商汤学术) </font></a>
                <br>
                <p align="justify" style="font-size:13px">We proposed Diverse Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization in data-free quantization, 
                    which obtained significant improvements over various networks and quantization methods.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/doam-o.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/2103.00809">
	             <papertitle>Over-sampling De-occlusion Attention Network for Prohibited Items Detection in Noisy X-ray Images</papertitle></a>
                 [<a href="https://arxiv.org/pdf/2103.00809.pdf">PDF</a>]
                 <br>Renshuai Tao, Yanlu Wei, Hainan Li, Aishan Liu, Yifu Ding, <strong>Haotong Qin</strong>, Xianglong Liu
                 <br>
                 <a href="https://arxiv.org/abs/2103.00809">arXiv, 2021</a> / 
                 <a href="https://github.com/OPIXray-author/OPIXray"><font color="red">Code</font></a> 
                 <iframe src="https://ghbtns.com/github-btn.html?user=OPIXray-author&repo=OPIXray&type=star&count=true&size=small"
                     frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 </p><p></p>
                 <p align="justify" style="font-size:13px">We contributed the first high-quality prohibited X-ray object detection dataset named OPIXray, 
                    and further proposed an over-sampling de-occlusion attention network (DOAM-O).</p>
            </td>
        </tr>

        
        <tr><td width="20%"><img src="./Imgs/clustering_icme2021.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href=" ">
                <papertitle>Multi-pretext Attention Network for Few-shot Learning with Self-supervision</papertitle></a>
                [<a href="https://arxiv.org/pdf/2103.05985">PDF</a>]
                <br>Hainan Li, Renshuai Tao, Jun Li, <strong>Haotong Qin</strong>, Yifu Ding, Shuo Wang, Xianglong Liu
                <br>
                <em>IEEE International Conference on Multimedia and Expo (ICME)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2103.05985">arXiv</a>
                </p><p></p>
                <p align="justify" style="font-size:13px">We proposed a Graph-driven Clustering (GC) for self-supervised learning, 
                    and Multi-pretext Attention Network (MAN) to combine the traditional augmentation-relied methods and our GC.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/cvpr2020_6014.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1909.10788">
                <papertitle>Forward and Backward Information Retention for Accurate Binary Neural Networks</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/QIN_CVPR2020_6014.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, Jingkuan Song
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1909.10788">arXiv</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/cF14wwgnMcnvkBa864ox1Q"><font color="red">(机器之心, </font></a>
                <a href="https://mp.weixin.qq.com/s/Sy42uvEpb6HANZqlXx1q0w"><font color="red">商汤学术, </font></a>
                <a href="https://mp.weixin.qq.com/s/I-MaaAmrcKd7-ATHmMtuwQ"><font color="red">CVer, </font></a>
                <a href="https://mp.weixin.qq.com/s/tglJjwCfAfa8UTydBMbqgA"><font color="red">AI科技大本营)</font></a> / 
                <a href="https://github.com/htqin/IR-Net"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=IR-Net&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We proposed a novel Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients,
                    and we were the first to implement and report 1-bit BNN speed on edge devices. </p>
            </td>
        </tr>


        <tr><td width="20%"><img src="./Imgs/pr2020_survey.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320300856">
                <papertitle>Binary Neural Network: A Survey</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/pr2020_BNN_survey.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu Sebe<br>
                <em>Pattern Recognition (PR)</em>, 2020 <font color="red"><strong>(IF=7.35)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2004.03333">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/QGva6fow9tad_daZ_G2p0Q"><font color="red">(PaperWeekly, </font></a>
                <a href="https://www.jiqizhixin.com/dailies/9c9fde93-8c87-4067-a4bd-f4815fafc49b"><font color="red">机器之心)</font></a> / 
                <a href="https://github.com/htqin/awesome-model-quantization"><font color="red">Code</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=awesome-model-quantization&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">In this paper, we presented a comprehensive survey of these algorithms. 
                    We also investigated other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. 
                    Then, we gave the evaluation and discussions on different tasks. 
                    Finally, the challenges that may be faced in future research were prospected.</p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/tmm2020_btvh.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://www.researchgate.net/publication/339823422_Boosting_Temporal_Binary_Coding_for_Large-scale_Video_Search">
	             <papertitle>Boosting Temporal Binary Coding for Large-scale Video Search</papertitle></a>
                 <br>Yan Wu, Xianglong Liu, <strong>Haotong Qin</strong>, Ke Xia, Sheng Hu, Yuqing Ma, Meng Wang
                 <br>
                 <em>IEEE Transactions on Multimedia (TMM)</em>, 2020 <font color="red"><strong>(IF=7.12)</strong></font>
                 <br>
                 </p><p align="justify" style="font-size:13px">In this paper, we first studied the multi-table learning problem for video search and attempted to
                     learn binary codes by capturing the intrinsic video similarities from both the visual and the temporal aspects.</p>
                <p></p>
            </td>
        </tr>
        </tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Honors</heading>
            <p> <strong>[2021.05]</strong> &nbsp;&nbsp; Huawei Scholarship (<strong>Top1%</strong>).</p>

             <p> <strong>[2020.09]</strong> &nbsp;&nbsp; China National Scholarship (<strong>Top2%</strong>).</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Enrolled in the Tencent Rhino-Bird Elite Training Program (<strong>51 people worldwide</strong>).</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Enrolled in the Shen Yuan Honors College at Beihang University (<strong>Top3%</strong>).</p>

             <p> <strong>[2019.04]</strong> &nbsp;&nbsp; The ICPC China National Invitational Contest (Nanchang, China) &nbsp;&nbsp;<strong>Gold Medal</strong>.</p>

             <p> <strong>[2018.03]</strong> &nbsp;&nbsp; The ACM-ICPC Chinese Collegiate Programming Contest (Shizuishan, China) &nbsp;&nbsp;<strong>Gold Medal</strong>.</p>

             <p> <strong>[2016.07]</strong> &nbsp;&nbsp; The International Concert of Chinese Folk Music (Kobe, Japan) &nbsp;&nbsp;<strong>Gold Medal</strong>.</p>

            </td>
            </tr></tbody>
    </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Talks & Academic Services</heading>
            <p style="font-size:13px"> <strong>[2020.04]</strong> Invited to present our <a href="https://arxiv.org/abs/1909.10788">IR-Net</a> and <a href="https://arxiv.org/abs/2004.03333">survey paper</a> at 
                JD AI Research. Here are the <a href="https://htqin.github.io/Slides/talk-JD-20200414.pptx">Slides</a>. </p>
            <p style="font-size:13px"> <strong>[2020-]</strong> I regularly review papers for top-tier conferences and journals in machine learning and computer vision.</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Teaching</heading>
            <p> <strong>[Fall 2020]</strong> &nbsp;&nbsp; Teaching Assistant in Machine Learning (Beihang University).</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like playing Chinese folk music, especially string music (Erhu, Zhonghu, etc.).
               In fact, I am almost a professional Erhu performer.
               I have studied Erhu supervised by Prof. <a href="https://www.ccom.edu.cn/jxyx/myx/myxls/201510/t20151026_35707.html">Zaili Tian</a>, Prof. <a href="https://baike.baidu.com/item/%E9%AB%98%E6%89%AC/5994407">Yang Gao</a>, and  Prof. Qingfu Zhu. 
               I was the vice-president of the Beihang Folk Music Orchestra, here are some of the performance videos of our orchestra [<a href="https://v.qq.com/x/search/?q=%E5%8C%97%E8%88%AA%E6%B0%91%E4%B9%90%E5%9B%A2&stag=txt.playpage.vppdesc">Tencent Video</a>].
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=O7AjyGXmrOPjngfk_VI9NhWFu7JWskLJlRerwKQdjh4"></script>
    </p></td>
    </tr>
    </tbody>
    </table>